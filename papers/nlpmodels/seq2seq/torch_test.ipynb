{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonirajalaa/Documents/fun/jonigrad/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = dict()\n",
    "    # Initialize vocabularies with special tokens\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in vocab: vocab[word.lower()] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def tokenize(vocab, data):\n",
    "    tok_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence_tok = []\n",
    "        sentence_tok.append(vocab['<SOS>'])\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in vocab:\n",
    "                sentence_tok.append(vocab[word.lower()])\n",
    "            else:\n",
    "                sentence_tok.append(vocab['<UNK>'])\n",
    "        sentence_tok.append(vocab['<EOS>'])\n",
    "        tok_data.append(sentence_tok)\n",
    "    \n",
    "    return tok_data\n",
    "\n",
    "def pad_sentences(data, pad_token):\n",
    "    max_len = max(len(sentence) for sentence in data)\n",
    "    padded_data = []\n",
    "    for sentence in data:\n",
    "        while len(sentence) < max_len:\n",
    "            sentence.append(pad_token)\n",
    "        padded_data.append(sentence)\n",
    "    return padded_data\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus_books\", 'en-fi')\n",
    "import numpy as np\n",
    "def dataset_to_numpy(dataset):\n",
    "    finnish_sentences = []\n",
    "    english_sentences = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        finnish_sentences.append(example['translation']['fi'])\n",
    "        english_sentences.append(example['translation']['en'])\n",
    "    \n",
    "    return finnish_sentences, english_sentences\n",
    "finnish_data, english_data = dataset_to_numpy(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab, fi_vocab = build_vocab(english_data), build_vocab(finnish_data)\n",
    "en_data_tok, fi_data_tok = tokenize(en_vocab, english_data), tokenize(fi_vocab, finnish_data)\n",
    "\n",
    "en_data_og = pad_sentences(en_data_tok, en_vocab['<PAD>'])\n",
    "fi_data_og = pad_sentences(fi_data_tok, fi_vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = np.array(en_data_og)\n",
    "fi_data = np.array(fi_data_og)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Initialize model, loss function, and optimizer\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m INPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mSRC\u001b[49m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m     62\u001b[0m OUTPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(TRG\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m     63\u001b[0m ENC_EMB_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SRC' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Encoder and Decoder classes\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Seq2Seq Model combining Encoder and Decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "INPUT_DIM = len(en_vocab)\n",
    "OUTPUT_DIM = len(fi_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Define data iterators, etc., and call the training loop\n",
    "# Here you would typically define your data loaders/iterators\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = dict()\n",
    "    # Initialize vocabularies with special tokens\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in vocab: vocab[word.lower()] = len(vocab)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def tokenize(vocab, data):\n",
    "    tok_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence_tok = []\n",
    "        sentence_tok.append(vocab['<SOS>'])\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in vocab:\n",
    "                sentence_tok.append(vocab[word.lower()])\n",
    "            else:\n",
    "                sentence_tok.append(vocab['<UNK>'])\n",
    "        sentence_tok.append(vocab['<EOS>'])\n",
    "        tok_data.append(sentence_tok)\n",
    "    \n",
    "    return tok_data\n",
    "\n",
    "def pad_sentences(data, pad_token):\n",
    "    max_len = max(len(sentence) for sentence in data)\n",
    "    padded_data = []\n",
    "    for sentence in data:\n",
    "        while len(sentence) < max_len:\n",
    "            sentence.append(pad_token)\n",
    "        padded_data.append(sentence)\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 26/114 [00:48<02:42,  1.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 184\u001b[0m\n\u001b[1;32m    181\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m--> 184\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 169\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m trg \u001b[38;5;241m=\u001b[39m trg[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[0;32m--> 169\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m    172\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/fun/jonigrad/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/fun/jonigrad/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/fun/jonigrad/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define special tokens\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
    "\n",
    "# Define functions to build vocabulary, tokenize, and pad sentences\n",
    "def build_vocab(data):\n",
    "    vocab = dict()\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "    for sentence in data:\n",
    "        for word in sentence.split():\n",
    "            if word.lower() not in vocab:\n",
    "                vocab[word.lower()] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def tokenize(vocab, data):\n",
    "    tok_data = []\n",
    "    for sentence in data:\n",
    "        sentence_tok = [vocab['<SOS>']]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in vocab:\n",
    "                sentence_tok.append(vocab[word.lower()])\n",
    "            else:\n",
    "                sentence_tok.append(vocab['<UNK>'])\n",
    "        sentence_tok.append(vocab['<EOS>'])\n",
    "        tok_data.append(sentence_tok)\n",
    "    return tok_data\n",
    "\n",
    "def pad_sentences(data, pad_token):\n",
    "    max_len = max(len(sentence) for sentence in data)\n",
    "    padded_data = []\n",
    "    for sentence in data:\n",
    "        while len(sentence) < max_len:\n",
    "            sentence.append(pad_token)\n",
    "        padded_data.append(sentence)\n",
    "    return padded_data\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"opus_books\", 'en-fi')\n",
    "\n",
    "def dataset_to_numpy(dataset):\n",
    "    finnish_sentences = []\n",
    "    english_sentences = []\n",
    "    for example in dataset:\n",
    "        finnish_sentences.append(example['translation']['fi'])\n",
    "        english_sentences.append(example['translation']['en'])\n",
    "    return finnish_sentences, english_sentences\n",
    "\n",
    "# Preprocess data\n",
    "finnish_data, english_data = dataset_to_numpy(dataset['train'])\n",
    "en_vocab, fi_vocab = build_vocab(english_data), build_vocab(finnish_data)\n",
    "en_data_tok, fi_data_tok = tokenize(en_vocab, english_data), tokenize(fi_vocab, finnish_data)\n",
    "\n",
    "en_data_og = pad_sentences(en_data_tok, en_vocab['<PAD>'])\n",
    "fi_data_og = pad_sentences(fi_data_tok, fi_vocab['<PAD>'])\n",
    "en_data = np.array(en_data_og)\n",
    "fi_data = np.array(fi_data_og)\n",
    "\n",
    "# Convert data to tensors and create dataloaders\n",
    "en_tensor = torch.tensor(en_data, dtype=torch.long)\n",
    "fi_tensor = torch.tensor(fi_data, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(en_tensor, fi_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define Encoder and Decoder classes\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Seq2Seq model combining Encoder and Decoder\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "INPUT_DIM = len(en_vocab)\n",
    "OUTPUT_DIM = len(fi_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab['<PAD>'])\n",
    "\n",
    "# Training loop\n",
    "def train(model, iterator, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(iterator, desc=\"Training\")):\n",
    "        src, trg = batch\n",
    "        src, trg = src.permute(1, 0).to(device), trg.permute(1, 0).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training setup\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, device)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, N_EPOCHS + 1), train_losses, marker='o', label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: hän on\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = [token.lower() for token in sentence.split()]\n",
    "    tokens = [src_vocab['<SOS>']] + [src_vocab.get(token, src_vocab['<UNK>']) for token in tokens] + [src_vocab['<EOS>']]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "    \n",
    "    trg_indexes = [trg_vocab['<SOS>']]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        \n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == trg_vocab['<EOS>']:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indexes]\n",
    "    \n",
    "    return trg_tokens[1:-1]\n",
    "\n",
    "# Translate a test sentence\n",
    "test_sentence = \"great thing\"\n",
    "translation = translate_sentence(test_sentence, en_vocab, fi_vocab, model, device)\n",
    "print(f'Translation: {\" \".join(translation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
